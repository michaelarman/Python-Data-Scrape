{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping with Python's BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining data is a very time consuming and difficult task but with web scraping we can make the task much faster and easier. \n",
    "This is a guide on how I scraped data from the OUA (Ontario University Athletics) website with Python and the Python library, BeautifulSoup. Doing this task requires some Python, and HTML knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at the website to get a feel of what data we want and where it is coming from.\n",
    "I was going through this site http://www.oua.ca/sports/mbkb/2018-19/leaders for the men's basketball teams and in the top right you can see a drop down menu where you can select the Season you want to view data from. This is how the webpage looks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/webpage.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After surfing the webpage for desired data, I came across a goldmine.\n",
    "In this table we can select a team and view many different statistics that were collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/table.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we click on a team we can find a game log for the entire regular season when we click on the 'Game Log' tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gamelog.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can see that each score has an embedded hyperlink and if we click it we can get the game stats. There are player stats of the game, play by play data, team stats, and 1st and 2nd half stats. What I was interested in was the team stats (I'll soon also scrape the player stats and possibly play-by-play data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/teamstats.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the goldmine right here. We have the game stats for each team and multiple useful tables that we can use. \n",
    "Now this is where the HTML comes in. What we want to do now is inspect elements on the page so that we can see the source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/htmltable.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we can see the HTML and its structure. In this page source we can see there are different 'sections'; these are the different tabs (e.g. Box Score tab, Play by Play tab, etc) and in each one there is a table. To scrape the data we need to figure out where the data comes from so that we can reference it and tell Python that we want it.\n",
    "\n",
    "In this table we have a lot of variables like Field Goal for Away and Home team, 3 Point, Turnovers, etc. So we want to find the tags that reference each of them and use some indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/indexing.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that the data is shown in the html, we can use Python to find the html tags and collect them.\n",
    "\n",
    "First we need to start with importing the appropriate library packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdictr = requests.get(url[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table>\n",
      "<caption class=\"caption offscreen\"><h2>Team Statistics</h2></caption>\n",
      "<tr>\n",
      "<th class=\"col-head\" scope=\"col\"><span class=\"offscreen\">Stat</span></th>\n",
      "<th class=\"col-head\" scope=\"col\">Algoma</th>\n",
      "<th class=\"col-head\" scope=\"col\">Laurentian</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<th class=\"row-head text\" scope=\"row\">Field Goal</th>\n",
      "<td>            31-64\n",
      "    </td>\n",
      "<td>            26-51\n",
      "    </td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th class=\"row-head text\" scope=\"row\">Field Goal %</th>\n",
      "<td>            48.4%\n",
      "    </td>\n",
      "<td>            51.0%\n",
      "    </td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th class=\"row-head text\" scope=\"row\">3 Point</th>\n",
      "<td>            11-23\n",
      "    </td>\n",
      "<td>            7-18\n",
      "    </td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th class=\"row-head text\" scope=\"row\">3 Point %</th>\n",
      "<td>            47.8%\n",
      "    </td>\n",
      "<td>            38.9%\n",
      "    </td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th class=\"row-head text\" scope=\"row\">Free Throw</th>\n",
      "<td>            8-9\n",
      "    </td>\n",
      "<td>            10-13\n",
      "    </td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th class=\"row-head text\" scope=\"row\">Free Throw %</th>\n",
      "<td>            88.9%\n",
      "    </td>\n",
      "<td>            76.9%\n",
      "    </td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th class=\"row-head text\" scope=\"row\">Rebounds</th>\n",
      "<td>34</td>\n",
      "<td>30</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th class=\"row-head text\" scope=\"row\">Assists</th>\n",
      "<td>12</td>\n",
      "<td>17</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th class=\"row-head text\" scope=\"row\">Turnovers</th>\n",
      "<td>6</td>\n",
      "<td>7</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th class=\"row-head text\" scope=\"row\">Points off Turnovers</th>\n",
      "<td>8</td>\n",
      "<td>2</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th class=\"row-head text\" scope=\"row\">2nd Chance Points</th>\n",
      "<td>18</td>\n",
      "<td>5</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th class=\"row-head text\" scope=\"row\">Bench Points</th>\n",
      "<td>17</td>\n",
      "<td>31</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th class=\"row-head text\" scope=\"row\">Largest Lead</th>\n",
      "<td>12</td>\n",
      "<td>6</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th class=\"row-head text\" scope=\"row\">Time of Largest Lead</th>\n",
      "<td>4th-00:25</td>\n",
      "<td>1st-06:07</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th class=\"row-head text\" scope=\"row\">Trends</th>\n",
      "<td colspan=\"2\">\n",
      "                            Ties: 9;\n",
      "                            Lead Changes: 10\n",
      "                        </td>\n",
      "</tr>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "r = requests.get('http://www.oua.ca/sports/mbkb/2018-19/boxscores/20180929_svqd.xml?view=teamstats')\n",
    "#this lets us access the webpage that we want to scrape from\n",
    "raw_html = r.content\n",
    "#we can then get the html\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "#soup is what we call the html of the webpage which we can use to scrape\n",
    "stats = soup.findAll(\"table\")\n",
    "print(stats[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the HTML of the table we want to scrape and we can index different tags to get the desired data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algoma\n",
      "Laurentian\n"
     ]
    }
   ],
   "source": [
    "table = stats[8]\n",
    "print(table.findAll('th', {'scope': 'col'})[1].text.strip()) #This will find the second \"th\" tag with the scope equal to \"col\" and this prints the Away team \n",
    "print(table.findAll('th', {'scope': 'col'})[2].text.strip()) #This will print the Home team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table is good but it doesn't provide the total scores from the game. However we do have the scores at the top of the page.\n",
    "\n",
    "<img src=\"images/score.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use BeautifulSoup to obtain these values as well. After figuring out where all the values are on the webpage, we can create a Python dictionary with keys as our variable names and values as our data that we obtain from the table. \n",
    "\n",
    "Below is a general function that scrapes the data of an OUA game webpage into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    \"\"\" This function is used to create data dictionaries for any url of team stats in the oua website. It\n",
    "    takes an array of urls but for some games there are extra fields to look out for. This function is for those that\n",
    "    do not have those extra fields in the table\"\"\"\n",
    "\n",
    "    # create dictionary for links with less fields in table\n",
    "    dictlist = {} # initializes a dictionary\n",
    "    for i in range(len(url)):\n",
    "    # loop through all the urls in an array\n",
    "        print(url[i])\n",
    "        r = requests.get(url[i])\n",
    "        raw_html = r.content\n",
    "        soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "        soup[url[i]] = BeautifulSoup(raw_html, 'html.parser')\n",
    "        stats = soup[url[i]].findAll(\"table\")\n",
    "        scores = soup[url[i]].findAll('div', {'class': 'teams clearfix'})[0].table\n",
    "        # some links have different amounts of tables and sometimes the team stats table is different\n",
    "        table = stats[8]\n",
    "        for j in range(2, len(stats)):\n",
    "            if str(stats[j].caption) == '<caption class=\"caption offscreen\"><h2>Team Statistics</h2></caption>':\n",
    "                table=stats[j]\n",
    "                break\n",
    "        # this makes sure we get the table we want since it will get the html with the certain caption class (a unique part of the html)\n",
    "\n",
    "        dictlist[url[i]] = {}\n",
    "        d = {}\n",
    "        # we will then create the dictionary with the proper keys and values\n",
    "        dictlist[url[i]] = {\n",
    "            \"Away\" : table.findAll('th', {'scope': 'col'})[1].text.strip(),\n",
    "            \"Home\" : table.findAll('th', {'scope': 'col'})[2].text.strip(),\n",
    "\n",
    "        }\n",
    "        # some webpages may not have the values listed below so we'll have to watch out for that and prevent getting an index error\n",
    "        try:\n",
    "            winner = scores.findAll('tr', {'class': 'winner'})[0]\n",
    "        except IndexError:\n",
    "            d[None] = None\n",
    "        # if the tag is not found we create a null key and value so that it doesn't break\n",
    "        try:\n",
    "            loser = scores.findAll('tr', {'class': 'loser'})[0]\n",
    "        except IndexError:\n",
    "            d[None] = None\n",
    "        try:\n",
    "            dictlist[url[i]].update({\"Winner\": winner.th.text.strip()})\n",
    "        except IndexError:\n",
    "            d[None] = None\n",
    "        try:\n",
    "            dictlist[url[i]].update({\"Loser\": loser.th.text.strip()})\n",
    "        except IndexError:\n",
    "            d[None] = None\n",
    "        try:\n",
    "            dictlist[url[i]].update({\"Winner 1st Qtr Pts\": winner.findAll('td')[0].text.strip()})\n",
    "        except IndexError:\n",
    "            d[None] = None\n",
    "        try:\n",
    "            dictlist[url[i]].update({\"Loser 1st Qtr Pts\": loser.findAll('td')[0].text.strip()})\n",
    "        except IndexError:\n",
    "            d[None] = None\n",
    "        try:\n",
    "            dictlist[url[i]].update({\"Winner 2nd Qtr Pts\": winner.findAll('td')[1].text.strip()})\n",
    "        except IndexError:\n",
    "            d[None] = None\n",
    "        try:\n",
    "            dictlist[url[i]].update({\"Loser 2nd Qtr Pts\": loser.findAll('td')[1].text.strip()})\n",
    "        except IndexError:\n",
    "            d[None] = None\n",
    "        try:\n",
    "            dictlist[url[i]].update({\"Winner 3rd Qtr Pts\": winner.findAll('td')[2].text.strip()})\n",
    "        except IndexError:\n",
    "            d[None] = None\n",
    "        try:\n",
    "            dictlist[url[i]].update({\"Loser 3rd Qtr Pts\": loser.findAll('td')[2].text.strip()})\n",
    "        except IndexError:\n",
    "            d[None] = None\n",
    "        try:\n",
    "            dictlist[url[i]].update({\"Winner 4th Qtr Pts\": winner.findAll('td')[3].text.strip()})\n",
    "        except IndexError:\n",
    "            d[None] = None\n",
    "        try:\n",
    "            dictlist[url[i]].update({\"Loser 4th Qtr Pts\": loser.findAll('td')[3].text.strip()})\n",
    "        except IndexError:\n",
    "            d[None] = None\n",
    "        try:\n",
    "            dictlist[url[i]].update( {\"Winner Total Pts\": winner.findAll('td')[4].text.strip()})\n",
    "        except IndexError:\n",
    "            d[None] = None\n",
    "        try:\n",
    "            dictlist[url[i]].update( {\"Loser Total Pts\": loser.findAll('td')[4].text.strip()})\n",
    "        except IndexError:\n",
    "            d[None] = None\n",
    "        for j in range(16):\n",
    "            try:\n",
    "                dictlist[url[i]].update( { table.findAll('th', {'scope': 'row'})[j].text.strip() + ' Away': table.findAll('td')[2*j].text.strip()})\n",
    "            except IndexError:\n",
    "                d[None] = None\n",
    "            try:\n",
    "                dictlist[url[i]].update({ table.findAll('th', {'scope': 'row'})[j].text.strip() + ' Home' : table.findAll('td')[2*j+1].text.strip()})\n",
    "            except IndexError:\n",
    "                d[None] = None\n",
    "            try:\n",
    "                dictlist[url[i]].update({table.findAll('th', {'scope': 'row'})[16].text.strip()+' Away': table.findAll('td')[32].text.strip()})\n",
    "            except IndexError:\n",
    "                d[None] = None\n",
    "    z = {**dictlist, **d}\n",
    "    # we then merge the two dictionaries so that it shows which values are missing\n",
    "    \n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function we can get the data from any array of game websites and it is generalized to prevent errors and obtain the right fields for the associated keys.\n",
    "\n",
    "Note: Some games can go to overtime but I found that an easy fix on Excel. If you sort the total points by least to greatest we can see that the quarters won't add up to the total so I created another column for the winner and losers and summed the entries to get the actual total.\n",
    "\n",
    "Collecting all the links would be time consuming as well so a function was made for that as well.\n",
    "We can use BeautifulSoup for this too since the html for the game log page contains a tag called href which is the url for the webpages that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links():\n",
    "    \"\"\" All links have a certain pattern since they are all similar. However there is a part of the url that makes \n",
    "     it unique and so we obtain the links from the html \"\"\"\n",
    "    print(\"getting links...\")\n",
    "    teams = ['algoma', 'brock', 'carleton', 'guelph', 'lakehead', 'laurentian',\n",
    "             'laurier', 'mcmaster', 'nipissing', 'ottawa', 'queens', 'ryerson',\n",
    "             'toronto', 'waterloo', 'western', 'windsor', 'york']\n",
    "    years = ['2014-15', '2015-16', '2016-17', '2017-18', '2018-19']\n",
    "    original_url = 'http://oua.ca/sports/mbkb/'\n",
    "    end_url = '?view=gamelog'\n",
    "    href_list = []\n",
    "    for year in years:\n",
    "        for team in teams:\n",
    "            current_url = original_url + year + '/teams/' + team + end_url\n",
    "            r = requests.get(current_url)\n",
    "            raw_html = r.content\n",
    "            soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "            tables = soup.findAll('table')\n",
    "            max_len = 0\n",
    "            index = 0\n",
    "\n",
    "            for i in range(len(tables)):\n",
    "                tags = tables[i].findAll('a')\n",
    "                if len(tags) > 0:\n",
    "                    url = tags[0].get('href', None)\n",
    "                    if \"/boxscores/20\" in url and len(tables[i]) > max_len:\n",
    "                        index = i\n",
    "                        max_len = len(tables[i])\n",
    "\n",
    "            table = tables[index]\n",
    "\n",
    "            tags = table.findAll('a')\n",
    "            for tag in tags:\n",
    "                url = re.sub(\"\\.\\.\", original_url + year, tag.get('href', None))\n",
    "                url += '?view=teamstats'\n",
    "                href_list.append(url)\n",
    "\n",
    "    print(\"done getting links\")\n",
    "    return href_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using both these functions together we can obtain our data and put it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    q = get_links()\n",
    "    a = scrape(q)\n",
    "    df = pd.DataFrame(a)\n",
    "    df = df.T\n",
    "    df = df.replace('\\-', ' -- ', regex=True).astype(object) #This is used because when opening in Excel \n",
    "    #it automatically assumes that the FG is a date (e.g. FG: 10-12)\n",
    "    df = df[['Away', 'FG Away', 'FG% Away', '3PT FG Away', '3PT FG% Away', 'FT Away', 'FT% Away', 'Rebounds Away',\n",
    "             'Assists Away',\n",
    "             'Turnovers Away', 'Points Off Turnovers Away', '2nd Chance Points Away', 'Points in the Paint Away',\n",
    "             'Fastbreak Points Away', 'Bench Points Away',\n",
    "             'Largest Lead Away', 'Time of Largest Lead Away', 'Home', 'FG Home',\n",
    "             'FG% Home', '3PT FG Home', '3PT FG% Home', 'FT Home', 'FT% Home', 'Rebounds Home', 'Assists Home',\n",
    "             'Turnovers Home',\n",
    "             'Points Off Turnovers Home', '2nd Chance Points Home', 'Points in the Paint Home', 'Fastbreak Points Home',\n",
    "             'Bench Points Home', 'Largest Lead Away', 'Time of Largest Lead Away', 'Trends Away', 'Winner', 'Winner 1st Qtr Pts',\n",
    "             'Winner 2nd Qtr Pts', 'Winner 3rd Qtr Pts', 'Winner 4th Qtr Pts', 'Winner Total Pts', 'Loser', 'Loser 1st Qtr Pts', 'Loser 2nd Qtr Pts',\n",
    "             'Loser 3rd Qtr Pts', 'Loser 4th Qtr Pts', 'Loser Total Pts']]\n",
    "\n",
    "    df.to_csv('gamebygame2.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/excel.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample of the output we got. There are 1171 rows and more than 50 columns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
